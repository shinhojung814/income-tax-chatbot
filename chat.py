import streamlit as st

from dotenv import load_dotenv
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_pinecone import PineconeVectorStore
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser

st.set_page_config(page_title="ì†Œë“ì„¸ ì±—ë´‡", page_icon="ğŸ¤–")

st.title("ğŸ¤– ì†Œë“ì„¸ ì±—ë´‡")
st.caption("ì†Œë“ì„¸ì— ê´€í•œ ì§ˆë¬¸ì„ ë‹µë³€í•´ë“œë¦½ë‹ˆë‹¤.")

load_dotenv()

if 'message_list' not in st.session_state:
    st.session_state.message_list = []

for message in st.session_state.message_list:
    with st.chat_message(message["role"]):
        st.write(message["content"])

def get_ai_message(user_message):
    # 1. ì„ë² ë”© ì„¤ì •
    embedding = OpenAIEmbeddings(model='text-embedding-3-large')

    # 2. ë°±í„° ë°ì´í„°ë² ì´ìŠ¤ ì„¤ì •
    index_name = 'tax-markdown-index'
    database = PineconeVectorStore.from_existing_index(index_name=index_name, embedding=embedding)

    # 3. LLM ì„¤ì •
    llm = ChatOpenAI(model='gpt-4o')

    # 4. ë‹µë³€ ìƒì„±ìš© í”„ë¡¬í”„íŠ¸ ì„¤ì •
    answer_prompt = ChatPromptTemplate.from_template("""[Identity]
    - ë‹¹ì‹ ì€ í•œêµ­ ì†Œë“ì„¸ ë²•ì˜ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
    - [Context]ë¥¼ ì°¸ê³ í•´ì„œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”.

    [Context]
    {context}

    ì§ˆë¬¸: {question}

    Answer:""")

    # 5. Retriever ì„¤ì •
    retriever = database.as_retriever(search_kwargs={"k": 4})

    # 6. QA ì²´ì¸ ì„¤ì •
    qa_chain = (
        {
            "context": lambda x: "\n\n".join(doc.page_content for doc in retriever.invoke(x["query"])),
            "question": lambda x: x["query"]
        }
        | answer_prompt
        | llm
        | StrOutputParser()
    )

    # 7. ì‚¬ì „ ì •ì˜
    dictionary = ["ì‚¬ëŒì„ ë‚˜íƒ€ë‚´ëŠ” í‘œí˜„ -> ê±°ì£¼ì"]

    # 8. ì§ˆë¬¸ ë³€í™˜ìš© í”„ë¡¬í”„íŠ¸ (ì‚¬ì „ ì ìš©)
    dictionary_prompt = ChatPromptTemplate.from_template("""ë‹¹ì‹ ì€ í•œêµ­ ì†Œë“ì„¸ ë²•ì˜ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

    ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ì½ê³ , ì•„ë˜ ì‚¬ì „ì„ ì°¸ê³ í•´ì„œ ì§ˆë¬¸ì„ ë” ì •í™•í•œ ë²•ë¥  ìš©ì–´ë¡œ ë³€ê²½í•´ì£¼ì„¸ìš”.
    ë§Œì•½ ë³€ê²½í•  í•„ìš”ê°€ ì—†ë‹¤ê³  íŒë‹¨ë˜ë©´ ì›ë˜ ì§ˆë¬¸ì„ ê·¸ëŒ€ë¡œ ë°˜í™˜í•´ì£¼ì„¸ìš”.

    ì‚¬ì „: {dictionary}

    ê¸°ì¡´ ì§ˆë¬¸: {question}

    ë³€ê²½ëœ ì§ˆë¬¸:""")

    # 9. ì§ˆë¬¸ ë³€í™˜ ì²´ì¸ (ì‚¬ì „ ì ìš©)
    dictionary_chain = (
        {
            "dictionary": lambda _: dictionary,
            "question": lambda x: x["question"]
        }
        |dictionary_prompt
        | llm
        | StrOutputParser()
    )

    # 10. ì†Œë“ì„¸ ì±—ë´‡ ì²´ì¸ ì„¤ì •
    tax_chain = {"query": dictionary_chain} | qa_chain
    ai_message = tax_chain.invoke({"question": user_message})

    return ai_message

if user_question := st.chat_input(placeholder="ì†Œë“ì„¸ì— ê´€í•œ ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”."):
    with st.chat_message("user"):
        st.write(user_question)
    st.session_state.message_list.append({"role": "user", "content": user_question})

    with st.spinner("ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ì¤‘ì…ë‹ˆë‹¤."):
        ai_message = get_ai_message(user_question)

        with st.chat_message("ai"):
            st.write(ai_message)
        st.session_state.message_list.append({"role": "ai", "content": ai_message})